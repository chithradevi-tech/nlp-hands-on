{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5738c836-c48e-46d9-a936-3166110a89cb",
   "metadata": {},
   "source": [
    "# Special Character Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9d6198-06ae-4a95-9ea4-5e04e83226d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are you\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#input string\n",
    "input_str = \"hello how are$ you!!\"\n",
    "\n",
    "#Using regular expressions to remove special characters\n",
    "clean_str = re.sub(r\"[^a-zA-Z0-9\\s]\",\"\",input_str)\n",
    "\n",
    "print(clean_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf10bc87-8c1c-46bb-aec8-0b5bc0625804",
   "metadata": {},
   "source": [
    "## Libraries in the field of NLP\n",
    "\n",
    "- SpaCy - Natural language processing library in Python that can be used to tokenize and process textual data\n",
    "- nltk (natural language toolkit)\n",
    "- !pip install -U spacy\n",
    "- !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "624cac1b-b675-4d13-8b5c-bef4be7b13c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are you\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input string\n",
    "input_str = \"hello how are$ you!!\"\n",
    "\n",
    "# Function to clean the string\n",
    "def clean_text(text):\n",
    "  cleaned_text = ''.join(char for char in text if char.isalpha() or char.isspace())\n",
    "  doc = nlp(cleaned_text)\n",
    "  return ' '.join(token.text for token in doc)\n",
    "\n",
    "# Get the final output\n",
    "clean_str = clean_text(input_str)\n",
    "print(clean_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc1d400-d6a5-4a67-9b5a-12c130ee74ff",
   "metadata": {},
   "source": [
    "### install nltk\n",
    "- !pip install nltk\n",
    "- import nltk\n",
    "  nltk.download('popular')\n",
    "  nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92eb8fcc-077f-47ee-b43a-b93f271b2e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5481b3db-b3cb-479a-a426-eba44f41931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK installed successfully!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(\"NLTK installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ff65f22-b2c7-4cf5-9d7e-74c814b88eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'how', 'are', '$', 'you', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "input_str = \"hello how are$ you!!\"\n",
    "tokens = word_tokenize(input_str)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ba39f9b-3637-451e-a03a-27756ab0fc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are you\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "\n",
    "input_str = \"hello how are$ you!!\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = nltk.word_tokenize(input_str)\n",
    "\n",
    "# Remove the special characters\n",
    "clean_tokens = [token for token in tokens if token.isalnum()]\n",
    "\n",
    "clean_str = ' '.join(clean_tokens)\n",
    "\n",
    "print(clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4637de65-a92d-4317-a259-e01b15d175fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
